C:\Users\LydoSr\anaconda3\envs\multimod\Lib\site-packages\transformers\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
2024-09-25 06:56:12,331 - INFO - Loaded audio embeddings from embeddings/audio_embeddings.pt.
2024-09-25 06:56:25,732 - INFO - Loaded transcriptions from E:\Ted\Text Chunks.
2024-09-25 06:56:25,851 - INFO - Loaded 123493 samples from the dataset.
c:\Users\LydoSr\Desktop\multimodal-llm\train.py:63: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()  # For mixed precision
Epoch 1/3
c:\Users\LydoSr\Desktop\multimodal-llm\train.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
C:\Users\LydoSr\anaconda3\envs\multimod\Lib\site-packages\transformers\models\gpt2\modeling_gpt2.py:544: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
  Batch 2 Loss: 9.1040
  Batch 4 Loss: 9.0008
  Batch 6 Loss: 9.1653
  Batch 8 Loss: 9.1533
  Batch 10 Loss: 9.0458
  Batch 12 Loss: 8.9202
  Batch 14 Loss: 8.9959
  Batch 16 Loss: 9.0138
  Batch 18 Loss: 9.0136
  Batch 20 Loss: 9.0308
  Batch 22 Loss: 8.9427
  Batch 24 Loss: 9.1832
  Batch 26 Loss: 8.9455
  Batch 28 Loss: 8.9430
  Batch 30 Loss: 9.0996
  Batch 32 Loss: 9.0411
  Batch 34 Loss: 9.2305
  Batch 36 Loss: 9.1510
  Batch 38 Loss: 8.7929
  Batch 40 Loss: 9.1607
Traceback (most recent call last):
  File "c:\Users\LydoSr\Desktop\multimodal-llm\train.py", line 125, in <module>
    train()
  File "c:\Users\LydoSr\Desktop\multimodal-llm\train.py", line 99, in train
    scaler.step(optimizer)
  File "C:\Users\LydoSr\anaconda3\envs\multimod\Lib\site-packages\torch\amp\grad_scaler.py", line 454, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\LydoSr\anaconda3\envs\multimod\Lib\site-packages\torch\amp\grad_scaler.py", line 351, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\LydoSr\anaconda3\envs\multimod\Lib\site-packages\torch\amp\grad_scaler.py", line 351, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
               ^^^^^^^^
KeyboardInterrupt
