C:\Users\LydoSr\anaconda3\envs\multimod\Lib\site-packages\transformers\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
2024-09-25 06:48:58,681 - INFO - Loaded audio embeddings from embeddings/audio_embeddings.pt.
2024-09-25 06:49:17,735 - INFO - Loaded transcriptions from E:\Ted\Text Chunks.
2024-09-25 06:49:17,858 - INFO - Loaded 123493 samples from the dataset.
c:\Users\LydoSr\Desktop\multimodal-llm\train.py:63: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()  # For mixed precision
Epoch 1/3
c:\Users\LydoSr\Desktop\multimodal-llm\train.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
C:\Users\LydoSr\anaconda3\envs\multimod\Lib\site-packages\transformers\models\gpt2\modeling_gpt2.py:544: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
C:\Users\LydoSr\anaconda3\envs\multimod\Lib\site-packages\torch\optim\lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
  Batch 8 Loss: 9.2965
  Batch 16 Loss: 8.9673
  Batch 24 Loss: 9.1961
  Batch 32 Loss: 9.2435
  Batch 40 Loss: 8.8477
  Batch 48 Loss: 9.2940
  Batch 56 Loss: 9.3430
  Batch 64 Loss: 8.7078
  Batch 72 Loss: 8.9192
  Batch 80 Loss: 9.6197
  Batch 88 Loss: 9.1019
  Batch 96 Loss: 9.0315
  Batch 104 Loss: 8.6435
  Batch 112 Loss: 8.5780
  Batch 120 Loss: 9.2440
  Batch 128 Loss: 9.0470
  Batch 136 Loss: 9.0478
  Batch 144 Loss: 9.3249
  Batch 152 Loss: 9.1293
  Batch 160 Loss: 8.5839
  Batch 168 Loss: 9.0621
  Batch 176 Loss: 8.9527
  Batch 184 Loss: 9.1588
  Batch 192 Loss: 8.8937
  Batch 200 Loss: 8.8011
  Batch 208 Loss: 8.9528
  Batch 216 Loss: 9.2333
  Batch 224 Loss: 8.8150
  Batch 232 Loss: 9.0313
  Batch 240 Loss: 9.1247
  Batch 248 Loss: 9.0516
  Batch 256 Loss: 9.2635
  Batch 264 Loss: 9.2566
  Batch 272 Loss: 8.7936
  Batch 280 Loss: 8.7943
  Batch 288 Loss: 8.8469
  Batch 296 Loss: 8.8624
  Batch 304 Loss: 9.0510
  Batch 312 Loss: 9.2021
  Batch 320 Loss: 7.6950
  Batch 328 Loss: 9.2415
  Batch 336 Loss: 9.1685
  Batch 344 Loss: 8.4887
  Batch 352 Loss: 9.3305
  Batch 360 Loss: 8.8465
  Batch 368 Loss: 8.1087
  Batch 376 Loss: 9.0986
  Batch 384 Loss: 8.8216
  Batch 392 Loss: 8.8407
  Batch 400 Loss: 8.6378
  Batch 408 Loss: 8.7115
  Batch 416 Loss: 8.9834
  Batch 424 Loss: 8.8991
  Batch 432 Loss: 9.3322
  Batch 440 Loss: 8.9121
  Batch 448 Loss: 9.0245
  Batch 456 Loss: 9.0168
  Batch 464 Loss: 9.3019
  Batch 472 Loss: 8.4028
  Batch 480 Loss: 8.9471
  Batch 488 Loss: 8.8942
  Batch 496 Loss: 9.1205
  Batch 504 Loss: 8.7954
  Batch 512 Loss: 8.5990
  Batch 520 Loss: 8.7404
  Batch 528 Loss: 8.6743
  Batch 536 Loss: 8.9155
  Batch 544 Loss: 8.8994
  Batch 552 Loss: 9.1531
  Batch 560 Loss: 8.8429
  Batch 568 Loss: 9.5211
  Batch 576 Loss: 8.9705
  Batch 584 Loss: 8.4719
  Batch 592 Loss: 9.1652
  Batch 600 Loss: 9.1514
  Batch 608 Loss: 7.9806
  Batch 616 Loss: 8.7232
  Batch 624 Loss: 8.9294
Traceback (most recent call last):
  File "c:\Users\LydoSr\Desktop\multimodal-llm\train.py", line 125, in <module>
    train()
  File "c:\Users\LydoSr\Desktop\multimodal-llm\train.py", line 91, in train
    scaler.scale(loss).backward()
  File "C:\Users\LydoSr\anaconda3\envs\multimod\Lib\site-packages\torch\_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "C:\Users\LydoSr\anaconda3\envs\multimod\Lib\site-packages\torch\autograd\__init__.py", line 289, in backward
    _engine_run_backward(
  File "C:\Users\LydoSr\anaconda3\envs\multimod\Lib\site-packages\torch\autograd\graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
